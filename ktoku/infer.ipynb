{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb5fe72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:07:30.571996Z",
     "iopub.status.busy": "2024-12-12T04:07:30.571341Z",
     "iopub.status.idle": "2024-12-12T04:12:13.423542Z",
     "shell.execute_reply": "2024-12-12T04:12:13.422483Z"
    },
    "papermill": {
     "duration": 282.861177,
     "end_time": "2024-12-12T04:12:13.426229",
     "exception": false,
     "start_time": "2024-12-12T04:07:30.565052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0\r\n",
      "Uninstalling torch-2.4.0:\r\n",
      "  Successfully uninstalled torch-2.4.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch\n",
    "!pip install -q --no-index --find-links=/kaggle/input/wheels-vllm-0-6-3-post1 torchvision==0.19.1\n",
    "!pip install -q --no-index --find-links=/kaggle/input/wheels-vllm-0-6-3-post1 vllm\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!pip install -q /kaggle/input/eedi-library/autoawq-0.2.7.post2-py3-none-any.whl --no-index --find-links=/kaggle/input/eedi-library \n",
    "!pip install -q /kaggle/input/eedi-library/peft-0.13.2-py3-none-any.whl --no-index --find-links=/kaggle/input/eedi-library \n",
    "!pip install -q /kaggle/input/eedi-library/bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl --no-index --find-links=/kaggle/input/eedi-library \n",
    "!pip install -q --no-deps --no-index /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ee3625",
   "metadata": {
    "papermill": {
     "duration": 0.004006,
     "end_time": "2024-12-12T04:12:13.435047",
     "exception": false,
     "start_time": "2024-12-12T04:12:13.431041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be228d62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:12:13.445057Z",
     "iopub.status.busy": "2024-12-12T04:12:13.444428Z",
     "iopub.status.idle": "2024-12-12T04:12:28.580055Z",
     "shell.execute_reply": "2024-12-12T04:12:28.579328Z"
    },
    "papermill": {
     "duration": 15.142972,
     "end_time": "2024-12-12T04:12:28.581985",
     "exception": false,
     "start_time": "2024-12-12T04:12:13.439013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, math, numpy as np\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re, gc\n",
    "import torch\n",
    "\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "df_train = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").fillna(-1).sample(10, random_state=42).reset_index(drop=True)\n",
    "df_test = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "df_misconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "\n",
    "df_ret = df_test.copy()\n",
    "\n",
    "TEMPLATE_INPUT_V3 = '{QUESTION}\\nCorrect answer: {CORRECT_ANSWER}\\nStudent wrong answer: {STUDENT_WRONG_ANSWER}'\n",
    "def format_input_v3(row, wrong_choice):\n",
    "\n",
    "    assert wrong_choice in \"ABCD\"\n",
    "    # Extract values from the row\n",
    "    question_text = row.get(\"QuestionText\", \"No question text provided\")\n",
    "    subject_name = row.get(\"SubjectName\", \"Unknown subject\")\n",
    "    construct_name = row.get(\"ConstructName\", \"Unknown construct\")\n",
    "    # Extract the correct and wrong answer text based on the choice\n",
    "    correct_answer = row.get(\"CorrectAnswer\", \"Unknown\")\n",
    "    assert wrong_choice != correct_answer\n",
    "    correct_answer_text = row.get(f\"Answer{correct_answer}Text\", \"No correct answer text available\")\n",
    "    wrong_answer_text = row.get(f\"Answer{wrong_choice}Text\", \"No wrong answer text available\")\n",
    "\n",
    "    # Construct the question format\n",
    "    formatted_question = f\"\"\"Question: {question_text}\n",
    "    \n",
    "SubjectName: {subject_name}\n",
    "ConstructName: {construct_name}\"\"\"\n",
    "\n",
    "    # Return the extracted data\n",
    "    ret = {\n",
    "        \"QUESTION\": formatted_question,\n",
    "        \"CORRECT_ANSWER\": correct_answer_text,\n",
    "        \"STUDENT_WRONG_ANSWER\": wrong_answer_text,\n",
    "        \"MISCONCEPTION_ID\": row.get('Misconception{wrong_choice}Id'),\n",
    "    }\n",
    "    ret[\"PROMPT\"] = TEMPLATE_INPUT_V3.format(**ret)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "items = []\n",
    "target_ids = []\n",
    "for _, row in df_ret.iterrows():\n",
    "    for choice in ['A', 'B', 'C', 'D']:\n",
    "        if choice == row[\"CorrectAnswer\"]:\n",
    "            continue\n",
    "            \n",
    "        correct_col = f\"Answer{row['CorrectAnswer']}Text\"\n",
    "        item = {'QuestionId_Answer': '{}_{}'.format(row['QuestionId'], choice)}\n",
    "        item['Prompt'] = format_input_v3(row, choice)['PROMPT']\n",
    "        items.append(item)\n",
    "        target_ids.append(int(row.get(f'Misconception{choice}Id', -1)))\n",
    "        \n",
    "df_input = pd.DataFrame(items)\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}'\n",
    "\n",
    "def get_detailed_example(task_description: str, query: str, response: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}\\n<response>{response}'\n",
    "\n",
    "def get_new_queries(queries, query_max_len, examples_prefix, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        queries,\n",
    "        max_length=query_max_len - len(tokenizer('<s>', add_special_tokens=False)['input_ids']) - len(\n",
    "            tokenizer('\\n<response></s>', add_special_tokens=False)['input_ids']),\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    prefix_ids = tokenizer(examples_prefix, add_special_tokens=False)['input_ids']\n",
    "    suffix_ids = tokenizer('\\n<response>', add_special_tokens=False)['input_ids']\n",
    "    new_max_length = (len(prefix_ids) + len(suffix_ids) + query_max_len + 8) // 8 * 8 + 8\n",
    "    new_queries = tokenizer.batch_decode(inputs['input_ids'])\n",
    "    for i in range(len(new_queries)):\n",
    "        new_queries[i] = examples_prefix + new_queries[i] + '\\n<response>'\n",
    "    return new_max_length, new_queries\n",
    "task =  \"Given a math multiple-choice problem with a student's wrong answer, retrieve the math misconceptions\"\n",
    "queries = [\n",
    "    get_detailed_instruct(task, q) for q in df_input['Prompt']\n",
    "]\n",
    "documents = df_misconception_mapping['MisconceptionName'].tolist()\n",
    "query_max_len, doc_max_len = 320, 48\n",
    "# LORA_PATH = '/kaggle/input/lora-14b-1126/transformers/default/1'\n",
    "LORA_PATH = '/kaggle/input/2211-lora-14b/transformers/default/1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(LORA_PATH)\n",
    "examples_prefix = ''\n",
    "new_query_max_len, new_queries = get_new_queries(queries, query_max_len, examples_prefix, tokenizer)\n",
    "\n",
    "\n",
    "import json\n",
    "with open('data.json', 'w') as f:\n",
    "    data = {'texts': new_queries+ documents}\n",
    "    f.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a7524d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:12:28.593409Z",
     "iopub.status.busy": "2024-12-12T04:12:28.592525Z",
     "iopub.status.idle": "2024-12-12T04:12:28.599916Z",
     "shell.execute_reply": "2024-12-12T04:12:28.598925Z"
    },
    "papermill": {
     "duration": 0.014012,
     "end_time": "2024-12-12T04:12:28.601503",
     "exception": false,
     "start_time": "2024-12-12T04:12:28.587491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_embed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_embed.py\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import peft\n",
    "\n",
    "MAX_LENGTH = 320\n",
    "\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[\n",
    "            torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_embeddings_in_batches(model, tokenizer, texts, max_length, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_dict = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        with torch.no_grad(), torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(**batch_dict)\n",
    "            batch_embeddings = last_token_pool(\n",
    "                outputs.last_hidden_state, batch_dict[\"attention_mask\"]\n",
    "            )\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1).cpu()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(base_model_path, lora_path, load_in_4bit=True):\n",
    "    model = AutoModel.from_pretrained(\n",
    "        base_model_path,\n",
    "        device_map=0,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        lora_path if lora_path else base_model_path\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    if lora_path:\n",
    "        model = peft.PeftModel.from_pretrained(model, lora_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    output_file = args.input_text.replace(\n",
    "        \".json\", \".pt.fold.{}.{}.embed\".format(*args.fold)\n",
    "    )\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Output file {output_file} already exists. Skipping...\")\n",
    "        return\n",
    "    model, tokenizer = load_model_and_tokenizer(\n",
    "        args.base_model, args.lora_path, load_in_4bit=args.load_in_4bit\n",
    "    )\n",
    "    texts = json.load(open(args.input_text))[\"texts\"][args.fold[0] :: args.fold[1]]\n",
    "    embeddings = get_embeddings_in_batches(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        batch_size=4,\n",
    "    )\n",
    "    text2embeds = {text: emb for text, emb in zip(texts, embeddings)}\n",
    "    torch.save(text2embeds, output_file)\n",
    "\n",
    "    del output_file, model, tokenizer, texts, embeddings, text2embeds\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--base_model\",\n",
    "        type=str,\n",
    "        default=\"Qwen/Qwen2.5-7B\",\n",
    "        help=\"Path to the base model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to the LoRA model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_text\",\n",
    "        type=str,\n",
    "        default=\".cache/data.json\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--load_in_4bit\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Load model in 4-bit mode\",\n",
    "    )\n",
    "    parser.add_argument(\"--fold\", nargs=2, type=int, default=[0, 1])\n",
    "    args = parser.parse_args()\n",
    "    if not os.path.exists(args.lora_path):\n",
    "        args.lora_path = None\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cea6b39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:12:28.611052Z",
     "iopub.status.busy": "2024-12-12T04:12:28.610392Z",
     "iopub.status.idle": "2024-12-12T04:18:14.252686Z",
     "shell.execute_reply": "2024-12-12T04:18:14.251699Z"
    },
    "papermill": {
     "duration": 345.649929,
     "end_time": "2024-12-12T04:18:14.255526",
     "exception": false,
     "start_time": "2024-12-12T04:12:28.605597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:55<00:00, 57.70s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:55<00:00, 57.87s/it]\n",
      "Embedding: 100%|██████████| 325/325 [03:13<00:00,  1.68it/s]\n",
      "Embedding:  97%|█████████▋| 314/325 [03:16<00:06,  1.63it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_path = '/kaggle/input/2211-lora-14b/transformers/default/1'\n",
    "cmd = f\"(CUDA_VISIBLE_DEVICES=0 python run_embed.py --base_model /kaggle/input/qw14b-awq/transformers/default/1 --lora_path {lora_path} --input_text data.json --fold 0 2) & (CUDA_VISIBLE_DEVICES=1 python run_embed.py --base_model /kaggle/input/qw14b-awq/transformers/default/1 --lora_path {lora_path} --input_text data.json --fold 1 2)\"\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50838f99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:18:14.309453Z",
     "iopub.status.busy": "2024-12-12T04:18:14.308661Z",
     "iopub.status.idle": "2024-12-12T04:18:23.390010Z",
     "shell.execute_reply": "2024-12-12T04:18:23.389110Z"
    },
    "papermill": {
     "duration": 9.107094,
     "end_time": "2024-12-12T04:18:23.392069",
     "exception": false,
     "start_time": "2024-12-12T04:18:14.284975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 325/325 [03:22<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.pt.fold.1.2.embed\n",
      "data.pt.fold.0.2.embed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/3626513216.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_to_embed.update(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import time\n",
    "text_to_embed = {}\n",
    "files = glob('*.pt*')\n",
    "while len(files) != 2:\n",
    "    time.sleep(1)\n",
    "    files = glob('*.pt*')\n",
    "\n",
    "\n",
    "time.sleep(3)    \n",
    "for path in files:\n",
    "    print(path)\n",
    "    text_to_embed.update(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce13920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:18:23.438953Z",
     "iopub.status.busy": "2024-12-12T04:18:23.438618Z",
     "iopub.status.idle": "2024-12-12T04:18:23.498061Z",
     "shell.execute_reply": "2024-12-12T04:18:23.497070Z"
    },
    "papermill": {
     "duration": 0.084895,
     "end_time": "2024-12-12T04:18:23.500169",
     "exception": false,
     "start_time": "2024-12-12T04:18:23.415274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_embeddings = torch.stack([text_to_embed[t] for t in new_queries])\n",
    "doc_embeddings = torch.stack([text_to_embed[t] for t in documents])\n",
    "\n",
    "scores = query_embeddings @ doc_embeddings.T  # Shape: (M, N)\n",
    "sorted_indices = torch.argsort(scores,1, descending=True)[:,:25].tolist()\n",
    "\n",
    "df_input[\"MisconceptionId\"] = [\" \".join([str(x) for x in row]) for row in sorted_indices]\n",
    "# df_input[[\"QuestionId_Answer\", \"MisconceptionId\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f16b3399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:18:23.548072Z",
     "iopub.status.busy": "2024-12-12T04:18:23.547429Z",
     "iopub.status.idle": "2024-12-12T04:18:23.644885Z",
     "shell.execute_reply": "2024-12-12T04:18:23.644017Z"
    },
    "papermill": {
     "duration": 0.123135,
     "end_time": "2024-12-12T04:18:23.646601",
     "exception": false,
     "start_time": "2024-12-12T04:18:23.523466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>ConstructId</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectId</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>CorrectAnswer</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>AnswerAText</th>\n",
       "      <th>AnswerBText</th>\n",
       "      <th>AnswerCText</th>\n",
       "      <th>AnswerDText</th>\n",
       "      <th>query_text</th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>answer_name</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>incorrect_answer</th>\n",
       "      <th>order_index</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "      <td>### SubjectName: BIDMAS\\n### ConstructName: Us...</td>\n",
       "      <td>1869_B</td>\n",
       "      <td>B</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>0</td>\n",
       "      <td>706 1507 1345 2306 328 1672 1005 2518 1963 253...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "      <td>### SubjectName: BIDMAS\\n### ConstructName: Us...</td>\n",
       "      <td>1869_C</td>\n",
       "      <td>C</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>1</td>\n",
       "      <td>2306 1507 706 1005 1345 1999 2488 2532 987 251...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "      <td>### SubjectName: BIDMAS\\n### ConstructName: Us...</td>\n",
       "      <td>1869_D</td>\n",
       "      <td>D</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "      <td>2</td>\n",
       "      <td>1005 328 1507 2532 1672 1516 706 1345 2306 248...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>D</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>### SubjectName: Simplifying Algebraic Fractio...</td>\n",
       "      <td>1870_A</td>\n",
       "      <td>A</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>3</td>\n",
       "      <td>2142 2068 167 891 418 1755 979 113 1421 320 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>D</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>### SubjectName: Simplifying Algebraic Fractio...</td>\n",
       "      <td>1870_B</td>\n",
       "      <td>B</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>4</td>\n",
       "      <td>2142 2068 167 891 341 979 1755 1871 143 418 11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QuestionId  ConstructId                                      ConstructName  \\\n",
       "0        1869          856  Use the order of operations to carry out calcu...   \n",
       "0        1869          856  Use the order of operations to carry out calcu...   \n",
       "0        1869          856  Use the order of operations to carry out calcu...   \n",
       "1        1870         1612  Simplify an algebraic fraction by factorising ...   \n",
       "1        1870         1612  Simplify an algebraic fraction by factorising ...   \n",
       "\n",
       "   SubjectId                      SubjectName CorrectAnswer  \\\n",
       "0         33                           BIDMAS             A   \n",
       "0         33                           BIDMAS             A   \n",
       "0         33                           BIDMAS             A   \n",
       "1       1077  Simplifying Algebraic Fractions             D   \n",
       "1       1077  Simplifying Algebraic Fractions             D   \n",
       "\n",
       "                                        QuestionText            AnswerAText  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "1  Simplify the following, if possible: \\( \\frac{...              \\( m+1 \\)   \n",
       "1  Simplify the following, if possible: \\( \\frac{...              \\( m+1 \\)   \n",
       "\n",
       "              AnswerBText            AnswerCText             AnswerDText  \\\n",
       "0  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  Does not need brackets   \n",
       "0  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  Does not need brackets   \n",
       "0  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  Does not need brackets   \n",
       "1               \\( m+2 \\)              \\( m-1 \\)       Does not simplify   \n",
       "1               \\( m+2 \\)              \\( m-1 \\)       Does not simplify   \n",
       "\n",
       "                                          query_text QuestionId_Answer  \\\n",
       "0  ### SubjectName: BIDMAS\\n### ConstructName: Us...            1869_B   \n",
       "0  ### SubjectName: BIDMAS\\n### ConstructName: Us...            1869_C   \n",
       "0  ### SubjectName: BIDMAS\\n### ConstructName: Us...            1869_D   \n",
       "1  ### SubjectName: Simplifying Algebraic Fractio...            1870_A   \n",
       "1  ### SubjectName: Simplifying Algebraic Fractio...            1870_B   \n",
       "\n",
       "  answer_name         correct_answer        incorrect_answer  order_index  \\\n",
       "0           B  \\( 3 \\times(2+4)-5 \\)  \\( 3 \\times 2+(4-5) \\)            0   \n",
       "0           C  \\( 3 \\times(2+4)-5 \\)   \\( 3 \\times(2+4-5) \\)            1   \n",
       "0           D  \\( 3 \\times(2+4)-5 \\)  Does not need brackets            2   \n",
       "1           A      Does not simplify               \\( m+1 \\)            3   \n",
       "1           B      Does not simplify               \\( m+2 \\)            4   \n",
       "\n",
       "                                     MisconceptionId  \n",
       "0  706 1507 1345 2306 328 1672 1005 2518 1963 253...  \n",
       "0  2306 1507 706 1005 1345 1999 2488 2532 987 251...  \n",
       "0  1005 328 1507 2532 1672 1516 706 1345 2306 248...  \n",
       "1  2142 2068 167 891 418 1755 979 113 1421 320 22...  \n",
       "1  2142 2068 167 891 341 979 1755 1871 143 418 11...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "full_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "\n",
    "rows = []\n",
    "for idx, row in full_df.iterrows():\n",
    "    for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        if option == row.CorrectAnswer:\n",
    "            continue\n",
    "            \n",
    "        correct_answer = row[f\"Answer{row.CorrectAnswer}Text\"]\n",
    "\n",
    "        query_text =f\"### SubjectName: {row['SubjectName']}\\n### ConstructName: {row['ConstructName']}\\n### Question: {row['QuestionText']}\\n### Correct Answer: {correct_answer}\\n### Misconcepte Incorrect answer: {option}.{row[f'Answer{option}Text']}\"\n",
    "        row[\"query_text\"] = query_text\n",
    "        row[\"QuestionId_Answer\"] = f\"{row.QuestionId}_{option}\"\n",
    "        row[\"answer_name\"] = option\n",
    "        row[\"correct_answer\"] = correct_answer\n",
    "        row[\"incorrect_answer\"] = row[f\"Answer{option}Text\"]\n",
    "        rows.append(copy.deepcopy(row))\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df['order_index'] = list(range(len(df)))\n",
    "df[\"MisconceptionId\"] = [\" \".join([str(x) for x in row]) for row in sorted_indices]\n",
    "df.to_parquet(\"df_target.parquet\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7147484c",
   "metadata": {
    "papermill": {
     "duration": 0.022483,
     "end_time": "2024-12-12T04:18:23.692621",
     "exception": false,
     "start_time": "2024-12-12T04:18:23.670138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LLM Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e68ceb20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:18:23.739889Z",
     "iopub.status.busy": "2024-12-12T04:18:23.739586Z",
     "iopub.status.idle": "2024-12-12T04:18:23.747485Z",
     "shell.execute_reply": "2024-12-12T04:18:23.746558Z"
    },
    "papermill": {
     "duration": 0.033765,
     "end_time": "2024-12-12T04:18:23.749160",
     "exception": false,
     "start_time": "2024-12-12T04:18:23.715395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vllm_logits.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm_logits.py\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vllm\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(\"http\\w+\", \"\", x)  # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)  # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()  # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"Here is a question about {ConstructName}({SubjectName}).\n",
    "Question: {Question}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "\n",
    "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\n",
    "Answer concisely what misconception it is to lead to getting the incorrect answer.\n",
    "Pick the correct misconception number from the below:\n",
    "\n",
    "{Retrival}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": preprocess_text(\n",
    "                PROMPT.format(\n",
    "                    ConstructName=row[\"ConstructName\"],\n",
    "                    SubjectName=row[\"SubjectName\"],\n",
    "                    Question=row[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row[\"incorrect_answer\"],\n",
    "                    CorrectAnswer=row[\"correct_answer\"],\n",
    "                    Retrival=row[\"retrieval\"],\n",
    "                )\n",
    "            ),\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "\n",
    "misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "\n",
    "df = pd.read_parquet(\"df_target.parquet\")\n",
    "indices = np.stack(df[\"MisconceptionId\"].apply(lambda x: np.array(list(map(int, x.split())))))\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.90,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "\n",
    "def get_candidates(c_indices):\n",
    "    candidates = []\n",
    "\n",
    "    mis_names = misconception_df[\"MisconceptionName\"].values\n",
    "    for ix in c_indices:\n",
    "        c_names = []\n",
    "        for i, name in enumerate(mis_names[ix]):\n",
    "            c_names.append(f\"{i+1}. {name}\")\n",
    "\n",
    "        candidates.append(\"\\n\".join(c_names))\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "survivors = indices[:, -1:]\n",
    "\n",
    "for i in range(3):\n",
    "    c_indices = np.concatenate([indices[:, -8 * (i + 1) - 1 : -8 * i - 1], survivors], axis=1)\n",
    "\n",
    "    df[\"retrieval\"] = get_candidates(c_indices)\n",
    "    df[\"text\"] = df.apply(lambda row: apply_template(row, tokenizer), axis=1)\n",
    "\n",
    "    responses = llm.generate(\n",
    "        df[\"text\"].values,\n",
    "        vllm.SamplingParams(\n",
    "            n=1,  # Number of output sequences to return for each prompt.\n",
    "            top_k=1,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "            temperature=0,  # randomness of the sampling\n",
    "            seed=777,  # Seed for reprodicibility\n",
    "            skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "            max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "            logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])],\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "    )\n",
    "\n",
    "    responses = [x.outputs[0].text for x in responses]\n",
    "    df[\"response\"] = responses\n",
    "\n",
    "    llm_choices = df[\"response\"].astype(int).values - 1\n",
    "\n",
    "    survivors = np.array([cix[best] for best, cix in zip(llm_choices, c_indices, strict=False)]).reshape(-1, 1)\n",
    "    df[f\"s{i}\"] = survivors\n",
    "\n",
    "\n",
    "def create_reranker_result(row):\n",
    "    originals = row.MisconceptionId.split()\n",
    "    rerank_result = [str(row.s2)] + originals[:8] + [str(row.s1)] + originals[8:16] + [str(row.s0)] + originals[16:]\n",
    "    rerank_result = list(dict.fromkeys(rerank_result))[:25]\n",
    "    return \" \".join(rerank_result)\n",
    "\n",
    "\n",
    "df[\"reranker_results\"] = df.apply(create_reranker_result, axis=1)\n",
    "\n",
    "##########################\n",
    "# 2,3位もLLMに抽出させる\n",
    "##########################\n",
    "\n",
    "\n",
    "# def extract_candidates(row, target_rank=2):\n",
    "#     target_ids = list(map(int, row.reranker_results.split()))[1:]\n",
    "#     if target_rank == 2:\n",
    "#         target_ids = target_ids[:9]\n",
    "#     if target_rank == 3:\n",
    "#         target_ids = [id for id in target_ids if id != row.f2][:9]\n",
    "#     return target_ids\n",
    "\n",
    "\n",
    "# for i in range(2):\n",
    "#     target_rank = i + 2\n",
    "#     df[\"candidates\"] = df.apply(lambda row: extract_candidates(row, target_rank=target_rank), axis=1)\n",
    "\n",
    "#     df[\"retrieval\"] = get_candidates(df[\"candidates\"].values)\n",
    "#     df[\"text\"] = df.apply(lambda row: apply_template(row, tokenizer), axis=1)\n",
    "#     responses = llm.generate(\n",
    "#         df[\"text\"].values,\n",
    "#         vllm.SamplingParams(\n",
    "#             n=1,  # Number of output sequences to return for each prompt.\n",
    "#             top_k=1,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "#             temperature=0,  # randomness of the sampling\n",
    "#             seed=777,  # Seed for reprodicibility\n",
    "#             skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "#             max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "#             logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])],\n",
    "#         ),\n",
    "#         use_tqdm=True,\n",
    "#     )\n",
    "\n",
    "#     responses = [x.outputs[0].text for x in responses]\n",
    "#     df[\"response\"] = responses\n",
    "\n",
    "#     llm_choices = df[\"response\"].astype(int).values - 1\n",
    "\n",
    "#     survivors = np.array([cix[best] for best, cix in zip(llm_choices, df[\"candidates\"].values, strict=False)]).reshape(-1, 1)\n",
    "#     df[f\"f{target_rank}\"] = survivors\n",
    "\n",
    "\n",
    "# def create_reranker_result_v2(row):\n",
    "#     originals = row.reranker_results.split()\n",
    "#     rerank_result = [originals[0]] + [str(row.f2), str(row.f3)] + originals[1:]\n",
    "#     rerank_result = list(dict.fromkeys(rerank_result))[:25]\n",
    "#     return \" \".join(rerank_result)\n",
    "\n",
    "# df[\"reranker_results_v2\"] = df.apply(create_reranker_result_v2, axis=1)\n",
    "\n",
    "##########################\n",
    "\n",
    "df.to_parquet(\"df_target.parquet\", index=False)\n",
    "df_sub = df[[\"QuestionId_Answer\", \"reranker_results\"]].copy()\n",
    "df_sub.columns = [\"QuestionId_Answer\", \"MisconceptionId\"]\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84c596a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:18:23.797435Z",
     "iopub.status.busy": "2024-12-12T04:18:23.796824Z",
     "iopub.status.idle": "2024-12-12T04:22:25.637102Z",
     "shell.execute_reply": "2024-12-12T04:22:25.635891Z"
    },
    "papermill": {
     "duration": 241.86797,
     "end_time": "2024-12-12T04:22:25.640162",
     "exception": false,
     "start_time": "2024-12-12T04:18:23.772192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-12 04:18:27 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\r\n",
      "WARNING 12-12 04:18:41 config.py:321] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-12 04:18:41 config.py:905] Defaulting to use mp for distributed inference\r\n",
      "WARNING 12-12 04:18:41 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 12-12 04:18:41 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\r\n",
      "WARNING 12-12 04:18:42 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 12-12 04:18:42 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m INFO 12-12 04:18:42 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-12 04:18:42 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-12 04:18:42 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m INFO 12-12 04:18:42 selector.py:115] Using XFormers backend.\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m INFO 12-12 04:18:43 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m INFO 12-12 04:18:44 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "INFO 12-12 04:18:44 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m INFO 12-12 04:18:44 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-12 04:18:44 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-12 04:18:44 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-12 04:19:02 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m INFO 12-12 04:19:02 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "WARNING 12-12 04:19:02 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m WARNING 12-12 04:19:02 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 12-12 04:19:02 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7cc15e70e800>, local_subscribe_port=47181, remote_subscribe_port=None)\r\n",
      "INFO 12-12 04:19:02 model_runner.py:1056] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m INFO 12-12 04:19:02 model_runner.py:1056] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "INFO 12-12 04:19:02 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-12 04:19:02 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m INFO 12-12 04:19:02 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m INFO 12-12 04:19:02 selector.py:115] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:23<01:35, 23.87s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:52<01:19, 26.45s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [01:29<01:02, 31.22s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [02:05<00:33, 33.20s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:42<00:00, 34.70s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:42<00:00, 32.52s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=218)\u001b[0;0m INFO 12-12 04:21:45 model_runner.py:1067] Loading model weights took 9.0933 GB\r\n",
      "INFO 12-12 04:21:46 model_runner.py:1067] Loading model weights took 9.0933 GB\r\n",
      "INFO 12-12 04:21:56 distributed_gpu_executor.py:57] # GPU blocks: 778, # CPU blocks: 2048\r\n",
      "INFO 12-12 04:21:56 distributed_gpu_executor.py:61] Maximum concurrency for 5120 tokens per request: 2.43x\r\n",
      "Processed prompts: 100%|█| 9/9 [00:07<00:00,  1.28it/s, est. speed input: 402.58\r\n",
      "Processed prompts: 100%|█| 9/9 [00:06<00:00,  1.37it/s, est. speed input: 449.58\r\n",
      "Processed prompts: 100%|█| 9/9 [00:06<00:00,  1.33it/s, est. speed input: 437.58\r\n"
     ]
    }
   ],
   "source": [
    "!python run_vllm_logits.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c26ec",
   "metadata": {
    "papermill": {
     "duration": 0.023777,
     "end_time": "2024-12-12T04:22:25.688808",
     "exception": false,
     "start_time": "2024-12-12T04:22:25.665031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LLM Reranker (finetuned 32B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "038f555c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:22:25.738658Z",
     "iopub.status.busy": "2024-12-12T04:22:25.738317Z",
     "iopub.status.idle": "2024-12-12T04:22:25.747212Z",
     "shell.execute_reply": "2024-12-12T04:22:25.746448Z"
    },
    "papermill": {
     "duration": 0.036235,
     "end_time": "2024-12-12T04:22:25.748950",
     "exception": false,
     "start_time": "2024-12-12T04:22:25.712715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_finetuned_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_finetuned_model.py\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, LogitsProcessor, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "model_path = \"/kaggle/input/m/qwen-lm/qwen2.5/transformers/32b-instruct/1\"\n",
    "CHECKPOINT_PATH = \"/kaggle/input/eedi-llm-hzghnvz\"\n",
    "misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "df = pd.read_parquet(\"df_target.parquet\")\n",
    "\n",
    "base_column_name = \"reranker_results\"\n",
    "indices = np.stack(df[base_column_name].apply(lambda x: np.array(list(map(int, x.split()))[:9])))\n",
    "indices_original = np.stack(df[base_column_name].apply(lambda x: np.array(list(map(int, x.split())))))\n",
    "\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(\"http\\w+\", \"\", x)  # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)  # Replace consecutive dots with a single dot\n",
    "    x = re.sub(r\"\\,+\", \",\", x)   # Replace consecutive commas with a single comma\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()  # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "PROMPT = \"\"\"<|im_start|>system\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "Here is a question about {ConstructName}({SubjectName}).\n",
    "Question: {Question}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "\n",
    "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\n",
    "Answer concisely what misconception it is to lead to getting the incorrect answer.\n",
    "Pick the correct misconception number from the below:\n",
    "\n",
    "{Retrival}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer:\"\"\"\n",
    "\n",
    "def apply_template(row, tokenizer):\n",
    "    return preprocess_text(PROMPT.format(\n",
    "        ConstructName=row[\"ConstructName\"],\n",
    "        SubjectName=row[\"SubjectName\"],\n",
    "        Question=row[\"QuestionText\"],\n",
    "        IncorrectAnswer=row[\"incorrect_answer\"],\n",
    "        CorrectAnswer=row[\"correct_answer\"],\n",
    "        Retrival=row[\"retrieval\"],\n",
    "    ))\n",
    "\n",
    "def get_candidates(c_indices):\n",
    "    candidates = []\n",
    "    mis_names = misconception_df[\"MisconceptionName\"].values\n",
    "    for ix in c_indices:\n",
    "        c_names = []\n",
    "        for i, name in enumerate(mis_names[ix]):\n",
    "            c_names.append(f\"{i+1}. {name}\")\n",
    "        candidates.append(\"\\n\".join(c_names))\n",
    "    return candidates\n",
    "\n",
    "# 推論用関数を定義\n",
    "def batched_inference(df, batch_size, tokenizer, model, generation_config, logits_processor):\n",
    "    responses = []\n",
    "    for start in range(0, len(df), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_df = df.iloc[start:end]\n",
    "        \n",
    "        # テキストをトークン化\n",
    "        input_texts = batch_df[\"text\"].values.tolist()\n",
    "        inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "        # 推論\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs, \n",
    "                generation_config=generation_config, \n",
    "                logits_processor=logits_processor\n",
    "            )\n",
    "        \n",
    "        # 結果をデコード\n",
    "        batch_responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        responses.extend([r.strip()[-1] for r in batch_responses])\n",
    "    return responses\n",
    "\n",
    "# ロジット処理用クラスの実装\n",
    "class MultipleChoiceLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer, choices):\n",
    "        super().__init__()\n",
    "        # choicesは[\"1\",\"2\",\"3\",...,\"9\"]といったシングルトークンを想定\n",
    "        self.choice_ids = []\n",
    "        for c in choices:\n",
    "            c_ids = tokenizer(c, add_special_tokens=False)[\"input_ids\"]\n",
    "            if len(c_ids) != 1:\n",
    "                raise ValueError(f\"Choice {c} is not a single token.\")\n",
    "            self.choice_ids.append(c_ids[0])\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # 全てのトークンを -inf でマスクし、選択肢のトークンのみ元のスコアを残す\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        for cid in self.choice_ids:\n",
    "            mask[..., cid] = scores[..., cid]\n",
    "        return mask\n",
    "\n",
    "########################### 量子化あり\n",
    "\n",
    "# 4bit量子化設定\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# トークナイザーとモデルのロード\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# LoRA を適用\n",
    "model = PeftModel.from_pretrained(model, CHECKPOINT_PATH)\n",
    "\n",
    "\n",
    "# 乱数シード設定（任意）\n",
    "torch.manual_seed(777)\n",
    "\n",
    "survivors = indices[:, -1:]\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=1,\n",
    "    do_sample=False,\n",
    "    top_k=1,\n",
    ")\n",
    "\n",
    "for i in range(1):\n",
    "    c_indices = np.concatenate([indices[:, -8 * (i + 1) - 1 : -8 * i - 1], survivors], axis=1)\n",
    "    df[\"retrieval\"] = get_candidates(c_indices)\n",
    "    df[\"text\"] = df.apply(lambda row: apply_template(row, tokenizer), axis=1)\n",
    "\n",
    "    if i == 0:\n",
    "        print('text 0')\n",
    "        print(df[\"text\"].values[0])\n",
    "\n",
    "    # ロジットプロセッサを設定\n",
    "    logits_processor = [MultipleChoiceLogitsProcessor(tokenizer, choices=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])]\n",
    "    \n",
    "    # バッチ推論\n",
    "    responses = batched_inference(df, BATCH_SIZE, tokenizer, model, generation_config, logits_processor)\n",
    "    df[\"response\"] = responses\n",
    "\n",
    "    # 回答を整数として変換\n",
    "    llm_choices = df[\"response\"].astype(int).values - 1\n",
    "    survivors = np.array([cix[best] for best, cix in zip(llm_choices, c_indices, strict=False)]).reshape(-1, 1)\n",
    "\n",
    "results = []\n",
    "for i in range(indices_original.shape[0]):\n",
    "    ix = indices_original[i]\n",
    "    llm_choice = survivors[i, 0]\n",
    "    results.append(\" \".join([str(llm_choice)] + [str(x) for x in ix if x != llm_choice]))\n",
    "\n",
    "df[\"reranker_results_v3\"] = results\n",
    "df.to_parquet(\"df_target.parquet\", index=False)\n",
    "df_sub = df[[\"QuestionId_Answer\", \"reranker_results_v3\"]].copy()\n",
    "df_sub.columns = [\"QuestionId_Answer\", \"MisconceptionId\"]\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a438319e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:22:25.799152Z",
     "iopub.status.busy": "2024-12-12T04:22:25.798831Z",
     "iopub.status.idle": "2024-12-12T04:30:54.384644Z",
     "shell.execute_reply": "2024-12-12T04:30:54.383305Z"
    },
    "papermill": {
     "duration": 508.613407,
     "end_time": "2024-12-12T04:30:54.387074",
     "exception": false,
     "start_time": "2024-12-12T04:22:25.773667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████| 17/17 [06:27<00:00, 22.82s/it]\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\r\n",
      "  warnings.warn(\r\n",
      "text 0\r\n",
      "<|im_start|>system\r\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Here is a question about Use the order of operations to carry out calculations involving powers(BIDMAS).\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "Correct Answer: 3 \\times(2+4)-5 \r\n",
      "Incorrect Answer: 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\r\n",
      "Answer concisely what misconception it is to lead to getting the incorrect answer.\r\n",
      "Pick the correct misconception number from the below:\r\n",
      "\r\n",
      "1. Inserts brackets but not changed order of operation\r\n",
      "2. Carries out operations from right to left regardless of priority order\r\n",
      "3. Carries out operations from left to right regardless of priority order\r\n",
      "4. Applies BIDMAS in strict order (does not realize addition and subtraction, and multiplication and division, are of equal priority)\r\n",
      "5. Performs addition ahead of multiplication\r\n",
      "6. Confuses the order of operations, believes addition comes before multiplication \r\n",
      "7. Carries out operations from left to right regardless of priority order, unless brackets are used\r\n",
      "8. May have made a calculation error using the order of operations\r\n",
      "9. Performs subtraction right to left if priority order means doing a calculation to the right first<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "Answer:\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\r\n",
      "  warnings.warn(\r\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\r\n"
     ]
    }
   ],
   "source": [
    "!python run_finetuned_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb64c59",
   "metadata": {
    "papermill": {
     "duration": 0.026031,
     "end_time": "2024-12-12T04:30:54.439848",
     "exception": false,
     "start_time": "2024-12-12T04:30:54.413817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Check Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf4b11e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:30:54.540615Z",
     "iopub.status.busy": "2024-12-12T04:30:54.539594Z",
     "iopub.status.idle": "2024-12-12T04:30:54.613453Z",
     "shell.execute_reply": "2024-12-12T04:30:54.612688Z"
    },
    "papermill": {
     "duration": 0.149438,
     "end_time": "2024-12-12T04:30:54.615241",
     "exception": false,
     "start_time": "2024-12-12T04:30:54.465803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>ConstructId</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectId</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>CorrectAnswer</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>AnswerAText</th>\n",
       "      <th>AnswerBText</th>\n",
       "      <th>AnswerCText</th>\n",
       "      <th>...</th>\n",
       "      <th>order_index</th>\n",
       "      <th>MisconceptionId</th>\n",
       "      <th>retrieval</th>\n",
       "      <th>text</th>\n",
       "      <th>response</th>\n",
       "      <th>s0</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>reranker_results</th>\n",
       "      <th>reranker_results_v3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>706 1507 1345 2306 328 1672 1005 2518 1963 253...</td>\n",
       "      <td>1. Inserts brackets but not changed order of o...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>3</td>\n",
       "      <td>1054</td>\n",
       "      <td>1963</td>\n",
       "      <td>1345</td>\n",
       "      <td>1345 706 1507 2306 328 1672 1005 2518 1963 253...</td>\n",
       "      <td>1507 1345 706 2306 328 1672 1005 2518 1963 253...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2306 1507 706 1005 1345 1999 2488 2532 987 251...</td>\n",
       "      <td>1. Inserts brackets but not changed order of o...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>3</td>\n",
       "      <td>2449</td>\n",
       "      <td>2518</td>\n",
       "      <td>1345</td>\n",
       "      <td>1345 2306 1507 706 1005 1999 2488 2532 2518 98...</td>\n",
       "      <td>1507 1345 2306 706 1005 1999 2488 2532 2518 98...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1005 328 1507 2532 1672 1516 706 1345 2306 248...</td>\n",
       "      <td>1. Believes order of operations does not affec...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>4</td>\n",
       "      <td>1941</td>\n",
       "      <td>315</td>\n",
       "      <td>2532</td>\n",
       "      <td>2532 1005 328 1507 1672 1516 706 1345 315 2306...</td>\n",
       "      <td>1507 2532 1005 328 1672 1516 706 1345 315 2306...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>D</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2142 2068 167 891 418 1755 979 113 1421 320 22...</td>\n",
       "      <td>1. Incorrectly cancels what they believe is a ...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>6</td>\n",
       "      <td>143</td>\n",
       "      <td>1871</td>\n",
       "      <td>891</td>\n",
       "      <td>891 2142 2068 167 418 1755 979 113 1871 1421 3...</td>\n",
       "      <td>1755 891 2142 2068 167 418 979 113 1871 1421 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>D</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2142 2068 167 891 341 979 1755 1871 143 418 11...</td>\n",
       "      <td>1. Incorrectly cancels what they believe is a ...</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>7</td>\n",
       "      <td>885</td>\n",
       "      <td>418</td>\n",
       "      <td>891</td>\n",
       "      <td>891 2142 2068 167 341 979 1755 1871 418 143 11...</td>\n",
       "      <td>1755 891 2142 2068 167 341 979 1871 418 143 11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   QuestionId  ConstructId                                      ConstructName  \\\n",
       "0        1869          856  Use the order of operations to carry out calcu...   \n",
       "1        1869          856  Use the order of operations to carry out calcu...   \n",
       "2        1869          856  Use the order of operations to carry out calcu...   \n",
       "3        1870         1612  Simplify an algebraic fraction by factorising ...   \n",
       "4        1870         1612  Simplify an algebraic fraction by factorising ...   \n",
       "\n",
       "   SubjectId                      SubjectName CorrectAnswer  \\\n",
       "0         33                           BIDMAS             A   \n",
       "1         33                           BIDMAS             A   \n",
       "2         33                           BIDMAS             A   \n",
       "3       1077  Simplifying Algebraic Fractions             D   \n",
       "4       1077  Simplifying Algebraic Fractions             D   \n",
       "\n",
       "                                        QuestionText            AnswerAText  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "1  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "2  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "3  Simplify the following, if possible: \\( \\frac{...              \\( m+1 \\)   \n",
       "4  Simplify the following, if possible: \\( \\frac{...              \\( m+1 \\)   \n",
       "\n",
       "              AnswerBText            AnswerCText  ... order_index  \\\n",
       "0  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  ...           0   \n",
       "1  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  ...           1   \n",
       "2  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  ...           2   \n",
       "3               \\( m+2 \\)              \\( m-1 \\)  ...           3   \n",
       "4               \\( m+2 \\)              \\( m-1 \\)  ...           4   \n",
       "\n",
       "                                     MisconceptionId  \\\n",
       "0  706 1507 1345 2306 328 1672 1005 2518 1963 253...   \n",
       "1  2306 1507 706 1005 1345 1999 2488 2532 987 251...   \n",
       "2  1005 328 1507 2532 1672 1516 706 1345 2306 248...   \n",
       "3  2142 2068 167 891 418 1755 979 113 1421 320 22...   \n",
       "4  2142 2068 167 891 341 979 1755 1871 143 418 11...   \n",
       "\n",
       "                                           retrieval  \\\n",
       "0  1. Inserts brackets but not changed order of o...   \n",
       "1  1. Inserts brackets but not changed order of o...   \n",
       "2  1. Believes order of operations does not affec...   \n",
       "3  1. Incorrectly cancels what they believe is a ...   \n",
       "4  1. Incorrectly cancels what they believe is a ...   \n",
       "\n",
       "                                                text response    s0    s1  \\\n",
       "0  <|im_start|>system\\nYou are Qwen, created by A...        3  1054  1963   \n",
       "1  <|im_start|>system\\nYou are Qwen, created by A...        3  2449  2518   \n",
       "2  <|im_start|>system\\nYou are Qwen, created by A...        4  1941   315   \n",
       "3  <|im_start|>system\\nYou are Qwen, created by A...        6   143  1871   \n",
       "4  <|im_start|>system\\nYou are Qwen, created by A...        7   885   418   \n",
       "\n",
       "     s2                                   reranker_results  \\\n",
       "0  1345  1345 706 1507 2306 328 1672 1005 2518 1963 253...   \n",
       "1  1345  1345 2306 1507 706 1005 1999 2488 2532 2518 98...   \n",
       "2  2532  2532 1005 328 1507 1672 1516 706 1345 315 2306...   \n",
       "3   891  891 2142 2068 167 418 1755 979 113 1871 1421 3...   \n",
       "4   891  891 2142 2068 167 341 979 1755 1871 418 143 11...   \n",
       "\n",
       "                                 reranker_results_v3  \n",
       "0  1507 1345 706 2306 328 1672 1005 2518 1963 253...  \n",
       "1  1507 1345 2306 706 1005 1999 2488 2532 2518 98...  \n",
       "2  1507 2532 1005 328 1672 1516 706 1345 315 2306...  \n",
       "3  1755 891 2142 2068 167 418 979 113 1871 1421 3...  \n",
       "4  1755 891 2142 2068 167 341 979 1871 418 143 11...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target = pd.read_parquet(\"df_target.parquet\")\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4995deb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:30:54.668677Z",
     "iopub.status.busy": "2024-12-12T04:30:54.668431Z",
     "iopub.status.idle": "2024-12-12T04:30:54.673992Z",
     "shell.execute_reply": "2024-12-12T04:30:54.673202Z"
    },
    "papermill": {
     "duration": 0.034152,
     "end_time": "2024-12-12T04:30:54.675985",
     "exception": false,
     "start_time": "2024-12-12T04:30:54.641833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706 1507 1345 2306 328 1672 1005 2518 1963 2532 1516 2488 2181 1999 1941 987 158 2449 234 15 1862 315 657 1054 77\n",
      "------------------\n",
      "1345 706 1507 2306 328 1672 1005 2518 1963 2532 1516 2488 2181 1999 1941 987 1054 158 2449 234 15 1862 315 657 77\n",
      "------------------\n",
      "1507 1345 706 2306 328 1672 1005 2518 1963 2532 1516 2488 2181 1999 1941 987 1054 158 2449 234 15 1862 315 657 77\n",
      "==================\n",
      "2306 1507 706 1005 1345 1999 2488 2532 987 2518 1672 328 1963 791 1516 1392 2392 2449 2181 1338 1214 2515 1248 158 657\n",
      "------------------\n",
      "1345 2306 1507 706 1005 1999 2488 2532 2518 987 1672 328 1963 791 1516 1392 2449 2392 2181 1338 1214 2515 1248 158 657\n",
      "------------------\n",
      "1507 1345 2306 706 1005 1999 2488 2532 2518 987 1672 328 1963 791 1516 1392 2449 2392 2181 1338 1214 2515 1248 158 657\n",
      "==================\n",
      "1005 328 1507 2532 1672 1516 706 1345 2306 2488 1392 2518 158 1999 1862 315 1856 15 2181 1941 1416 2449 1319 2326 987\n",
      "------------------\n",
      "2532 1005 328 1507 1672 1516 706 1345 315 2306 2488 1392 2518 158 1999 1862 1941 1856 15 2181 1416 2449 1319 2326 987\n",
      "------------------\n",
      "1507 2532 1005 328 1672 1516 706 1345 315 2306 2488 1392 2518 158 1999 1862 1941 1856 15 2181 1416 2449 1319 2326 987\n",
      "==================\n",
      "2142 2068 167 891 418 1755 979 113 1421 320 2256 2143 2078 1871 885 265 1535 1904 143 2549 341 1593 1388 683 220\n",
      "------------------\n",
      "891 2142 2068 167 418 1755 979 113 1871 1421 320 2256 2143 2078 885 265 143 1535 1904 2549 341 1593 1388 683 220\n",
      "------------------\n",
      "1755 891 2142 2068 167 418 979 113 1871 1421 320 2256 2143 2078 885 265 143 1535 1904 2549 341 1593 1388 683 220\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df_target.iterrows():\n",
    "    print(row.MisconceptionId)\n",
    "    print('------------------')\n",
    "    print(row.reranker_results)\n",
    "    print('------------------')\n",
    "    # print(row.reranker_results_v2)\n",
    "    # print('------------------')\n",
    "    print(row.reranker_results_v3)\n",
    "    print('==================')\n",
    "    if idx >= 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2144f1d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T04:30:54.728788Z",
     "iopub.status.busy": "2024-12-12T04:30:54.728297Z",
     "iopub.status.idle": "2024-12-12T04:30:54.738910Z",
     "shell.execute_reply": "2024-12-12T04:30:54.737939Z"
    },
    "papermill": {
     "duration": 0.038621,
     "end_time": "2024-12-12T04:30:54.740539",
     "exception": false,
     "start_time": "2024-12-12T04:30:54.701918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>1507 1345 706 2306 328 1672 1005 2518 1963 253...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>1507 1345 2306 706 1005 1999 2488 2532 2518 98...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>1507 2532 1005 328 1672 1516 706 1345 315 2306...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>1755 891 2142 2068 167 418 979 113 1871 1421 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>1755 891 2142 2068 167 341 979 1871 418 143 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>1755 891 2142 2068 167 418 113 2078 265 143 97...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>1287 1073 2439 1665 2551 1306 1059 1098 1866 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>1287 1073 2439 1665 2551 1098 1059 912 1866 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1073 1287 1059 1866 903 2471 912 2439 1975 206...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  1507 1345 706 2306 328 1672 1005 2518 1963 253...\n",
       "1            1869_C  1507 1345 2306 706 1005 1999 2488 2532 2518 98...\n",
       "2            1869_D  1507 2532 1005 328 1672 1516 706 1345 315 2306...\n",
       "3            1870_A  1755 891 2142 2068 167 418 979 113 1871 1421 3...\n",
       "4            1870_B  1755 891 2142 2068 167 341 979 1871 418 143 11...\n",
       "5            1870_C  1755 891 2142 2068 167 418 113 2078 265 143 97...\n",
       "6            1871_A  1287 1073 2439 1665 2551 1306 1059 1098 1866 1...\n",
       "7            1871_C  1287 1073 2439 1665 2551 1098 1059 912 1866 13...\n",
       "8            1871_D  1073 1287 1059 1866 903 2471 912 2439 1975 206..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6117312,
     "sourceId": 9948011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6255429,
     "sourceId": 10135731,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6260648,
     "sourceId": 10143084,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6271304,
     "sourceId": 10157080,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10160879,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6279144,
     "sourceId": 10167786,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6281399,
     "sourceId": 10170730,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6281550,
     "sourceId": 10170921,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6284842,
     "sourceId": 10175321,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6284879,
     "sourceId": 10175386,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6285389,
     "sourceId": 10176061,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 202903897,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 210301909,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 123481,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 165390,
     "modelInstanceId": 142811,
     "sourceId": 167864,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 145133,
     "sourceId": 170579,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171421,
     "modelInstanceId": 148911,
     "sourceId": 174909,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171434,
     "modelInstanceId": 148923,
     "sourceId": 174921,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1409.195346,
   "end_time": "2024-12-12T04:30:57.269409",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-12T04:07:28.074063",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
